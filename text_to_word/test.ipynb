{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6982cd06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f630ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/202044005/.conda/envs/KSEB/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-25 22:20:59.789571: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-25 22:20:59.806497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756128059.823916 3529888 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756128059.829208 3529888 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756128059.847510 3529888 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756128059.847534 3529888 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756128059.847536 3529888 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756128059.847537 3529888 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-25 22:20:59.852252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bd06c17ad30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "#nn: 신경망을 구축하는데 필요한 모듈, \n",
    "import torch.nn as nn\n",
    "# functional: 활성화 함수나 풀링 등 레이어들과 유사한 기능을 수행하는 함수 제공\n",
    "import torch.nn.functional as F\n",
    "# transformers: 허깅페이스(Hugging Face)에서 제공하는 라이브러리로, \n",
    "    # 최신 트랜스포머 기반 모델들을 매우 쉽게 사용할 수 있도록 돕는 강력한 도구\n",
    "# T5ForConditionalGeneration: 번역, 요약과 같은 '조건부 생성' 작업을 위해 특별히 설계된 클래스\n",
    "# T5Tokenizer: T5 모델을 위한 전용 토크나이저입니다.\n",
    "# Trainer: 델, 훈련 설정, 데이터셋 등을 입력받아 전체 훈련 및 평가 과정을 자동으로 처리해주는 고수준 API입니다.\n",
    "# TrainingArguments : 모델 훈련에 필요한 모든 하이퍼파라미터(학습률, 배치 크기, 에폭 수, 저장 경로 등)를 정의하는 설정 클래스입니다\n",
    "# PreTrainedModel: 허깅페이스의 모든 사전 훈련된 모델이 상속하는 기본 클래스입니다.\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, PreTrainedModel\n",
    "# 퀀스-투-시퀀스(Seq2Seq) 모델의 출력값을 체계적으로 정리해주는 데이터 구조\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "# Dataset: 사용자 정의 데이터셋을 만들기 위한 추상 클래스입니다.\n",
    "# DataLoader: 지정된 배치 크기(batch size)에 맞게 데이터를 묶어주고, 데이터를 섞거나(shuffle), 여러 개의 CPU 코어를 사용해 병렬로 데이터를 불러오는 등 훈련 과정을 효율적으로 만들어주는 핵심적인 유틸리티입니다.\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 구글에서 개발한 비지도(unsupervised) 텍스트 토크나이저 라이브러리입니다. BPE, Unigram 등 다양한 서브워드(subword) 분절 알고리즘을 지원하며, 특정 언어에 종속되지 않는다는 장점이 있습니다.\n",
    "import sentencepiece as spm\n",
    "# 운영체제(Operating System)와 상호작용할 수 있는 다양한 기능을 제공하는 파이썬 표준 라이브러리입니다.\n",
    "import os\n",
    "\n",
    "# 재현성을 위한 시드 고정\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605fb488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맞춤형 토크나이저를 훈련합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=resumes.txt --model_prefix=resume_tokenizer --vocab_size=32000 --model_type=bpe --character_coverage=0.9995 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: resumes.txt\n",
      "  input_format: \n",
      "  model_prefix: resume_tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 32000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: resumes.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 3 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=92\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 3 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 3\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 17\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=20 all=131 active=68 piece=▁컴\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=40 all=135 active=72 piece=▁근무\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=60 all=127 active=64 piece=▁엔지니어로\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=80 all=107 active=44 piece=설계\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=100 all=87 active=24 piece=▁경험\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=120 all=67 active=4 piece=성전자에서\n",
      "bpe_model_trainer.cc(252) LOG(WARNING) No valid symbol found\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: resume_tokenizer.model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Internal: src/trainer_interface.cc(662) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (32000). Please set it to a value <= 190.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m파이썬과 PyTorch를 활용한 딥러닝 모델 개발 경험이 있습니다.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m국민대학교 컴퓨터공학부를 졸업했습니다.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtrain_custom_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_data_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 훈련된 토크나이저 로드\u001b[39;00m\n\u001b[1;32m     42\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer(vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume_tokenizer.model\u001b[39m\u001b[38;5;124m'\u001b[39m, extra_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mtrain_custom_tokenizer\u001b[0;34m(data_file, model_prefix, vocab_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.model\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m맞춤형 토크나이저를 훈련합니다...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mspm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--input=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_file\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m --model_prefix=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_prefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--vocab_size=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvocab_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m --model_type=bpe \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--character_coverage=0.9995 \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m토크나이저 훈련 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/KSEB/lib/python3.10/site-packages/sentencepiece/__init__.py:1047\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mTrain\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logstream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1046\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream\u001b[38;5;241m=\u001b[39mlogstream):\n\u001b[0;32m-> 1047\u001b[0m     \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/KSEB/lib/python3.10/site-packages/sentencepiece/__init__.py:1003\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train Sentencepiece model. Accept both kwargs and legacy string arg.\"\"\"\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(arg) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m-> 1003\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_TrainFromString\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_encode\u001b[39m(value):\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Encode value to CSV..\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/KSEB/lib/python3.10/site-packages/sentencepiece/__init__.py:981\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromString\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_TrainFromString\u001b[39m(arg):\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer__TrainFromString\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Internal: src/trainer_interface.cc(662) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (32000). Please set it to a value <= 190."
     ]
    }
   ],
   "source": [
    "# --- 2.1. 이력서 텍스트 파일 준비 (예시) ---\n",
    "# 'resumes.txt' 파일에 이력서 관련 문장들을 한 줄에 하나씩 저장합니다.\n",
    "# 예: \"삼성전자에서 반도체 설계 엔지니어로 3년간 근무했습니다.\"\n",
    "#     \"파이썬과 PyTorch를 활용한 딥러닝 모델 개발 경험이 있습니다.\"\n",
    "\n",
    "# --- 2.2. SentencePiece 토크나이저 훈련 ---\n",
    "def train_custom_tokenizer(data_file, model_prefix='resume_tokenizer', vocab_size=32000):\n",
    "    '''\n",
    "        data_file: Required. Path to the text file to train the torquerizer on.\n",
    "            필수 입력값. 토크나이저를 훈련시킬 텍스트 파일의 경로\n",
    "        model_prefix: Optional input. The name of the model file that will be generated after training. The default value is set to ‘resume_tokenizer’\n",
    "            선택 입력값. 훈련 후 생성될 모델 파일의 이름입니다. 기본값은 'resume_tokenizer'로 설정되어 있습니다.\n",
    "\n",
    "        vocab_size: Optional input. Determines the size of the word field. The default is 32000.\n",
    "            선택 입력값. 단어장의 크기를 결정합니다. 기본값은 32000개 입니다.\n",
    "    '''\n",
    "    if not os.path.exists(f'{model_prefix}.model'):\n",
    "        print(\"맞춤형 토크나이저를 훈련합니다...\")\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            f'--input={data_file} --model_prefix={model_prefix} '\n",
    "            f'--vocab_size={vocab_size} --model_type=bpe '\n",
    "            f'--character_coverage=0.9995 '\n",
    "            f'--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3'\n",
    "        )\n",
    "        print(\"토크나이저 훈련 완료!\")\n",
    "    else:\n",
    "        print(\"훈련된 토크나이저를 사용합니다.\")\n",
    "\n",
    "# 이력서 데이터 파일 경로\n",
    "resume_data_file = 'resumes.txt' \n",
    "# # 이력서 데이터가 없다면, 임시 파일 생성\n",
    "if not os.path.exists(resume_data_file):\n",
    "    with open(resume_data_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"삼성전자에서 반도체 설계 엔지니어로 3년간 근무했습니다.\\n\")\n",
    "        f.write(\"파이썬과 PyTorch를 활용한 딥러닝 모델 개발 경험이 있습니다.\\n\")\n",
    "        f.write(\"국민대학교 컴퓨터공학부를 졸업했습니다.\\n\")\n",
    "\n",
    "\n",
    "train_custom_tokenizer(resume_data_file)\n",
    "\n",
    "# 훈련된 토크나이저 로드\n",
    "tokenizer = T5Tokenizer(vocab_file='resume_tokenizer.model', extra_ids=0)\n",
    "\n",
    "# --- 2.3. 지문자 처리를 위한 자모(Jamo) 토큰 추가 ---\n",
    "# 한글 자모와 같은 특수 토큰들을 사전에 추가합니다.\n",
    "korean_jamo = [\n",
    "    'ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ',\n",
    "    'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ'\n",
    "]\n",
    "\n",
    "# korean_jamo = [\n",
    "#     'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ',\n",
    "#     'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ'\n",
    "# ]\n",
    "tokenizer.add_tokens(korean_jamo)\n",
    "print(f\"새로운 토큰 추가 후 토크나이저 크기: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad5fd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1.2, 1.4, 1.6, 1.8, 2. , 2.2, 2.4, 2.6, 2.8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(1.0, 3.0, 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
